---
geometry:
- top=25mm
- left=20mm
- right=20mm
- bottom=30mm
documentclass: extarticle
fontsize: 12pt
numbersections: true
title: Lecture 17 (Cache Optimizations)
--- 

# Pipelined Cache

## Tasks Involved - Read
1. Tag array access || Data array access
1. Tag comparison
1. Data Block selection

## Tasks Involved - Write
1. Access tag array
1. Tag comparison
1. Data write

# Non-Blocking Cache
1. Cache miss should not stall later accesses
1. Miss Status Holding Register is used
1. Maintains read/write bit, word address, tag, store value
1. Miss queue is maintained in FIFO order
1. Can minimize the number of miss requests sent to lower levels


# Skewed Associative Cache
4 Cuckoo hashes are used

## Cuckoo Hashing
1. Have two hash functions
1. Check either of the two locations for lookup or deletion
1. For insertion, if collision, remove old entry and insert it in the other one

# Way Prediction
1. Reading all tags and comparing is inefficient
1. Predict the way
1. Compare that first
1. Else search all

## Technique
1. Steps: read registers, compute address, check for LSQ forwarding, access d-cache
1. Meanwhile compute $r1\oplus 12$ (instruction = ld r2, 12[r1]) and get way via the way predictor
1. Prediction available when instruction at d-cache

# Loop Tiling
*pappu code for matrix multiplication; this is used 99% of the time*

1. Issue with row-major vs column-major - row better for first matrix, column for second matrix
1. Can instead have tiling in the loop
1. Smaller blocks are multiplied
1. Can store these rectangles in cache

# Virtually Indexed Physically Tagged (VIPT) Caches
1. We have been ignoring address translation
1. Can use the offset for index and byte offset
1. Page ID and frame ID are like the tag, so this translation can be done in parallel
1. This limits the number of sets in cache
